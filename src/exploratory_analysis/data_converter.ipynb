{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the size of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Separation of queries: \n",
    "We should find a way to return graphs of posts, users and subreddits, one subreddit at a time. This is computationally easier to handle for our pc's. The queries will be simpler, and it will be more manual work. However once we have all these separate files we can merge them together into a big connected graph. It is not clear to me yet which approach is easiest.\n",
    "1.  Exporting all the data in a csv and merging the csv's together and importing the data into gephi OR\n",
    "2.  Exporting the graphml files and merging them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Reducing Complexity of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 User short list\n",
    "We generated two approaches to arrive to a shortlist of users to analyse\n",
    "\n",
    "#### 2.1.1 Subreddit driven approach: \n",
    "For each of the three main corona subreddits we need to return the top 100 posts with the highest karma. For these 300 posts we extract the users that made them. These are intensive reddit users that have a high Impact within these three subreddits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NEO4J QUERY**\n",
    "\n",
    "**coronavirus**\n",
    "```\n",
    "MATCH (u:User)--(p:Post)--(s:Subreddit {display_name:'coronavirus'})\n",
    "RETURN p.title, p.score,  s.display_name,u.username\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**covid19**\n",
    "```\n",
    "MATCH (u:User)--(p:Post)--(s:Subreddit {display_name:'coronavirus'})\n",
    "RETURN p.title, p.score,  s.display_name,u.username\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**china_flu**\n",
    "```\n",
    "MATCH (u:User)--(p:Post)--(s:Subreddit {display_name:'coronavirus'})\n",
    "RETURN p.title, p.score,  s.display_name,u.username\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 100\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function defintions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates(df):\n",
    "    \"Takes UTC dates and returns columns for the months, weeks \"\n",
    "    df['month'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.month_name()\n",
    "    df['month_n'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.month\n",
    "    df['day'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.day_name()\n",
    "    df['day_y'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.dayofyear\n",
    "    df['day_w'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.dayofweek\n",
    "    df['week_y'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.weekofyear\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract list of users from the three main corona subreddits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users list shape: (300, 4)\n"
     ]
    }
   ],
   "source": [
    "# Setting up data DIRS\n",
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\users'\n",
    "INPUT_DIR = '\\\\input'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "VIS_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\outputs\\\\exploratory_analysis\\\\users\\\\'\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "\n",
    "# Loading users in dataframe\n",
    "files = ['china_flu','coronavirus','covid19']\n",
    "users = pd.DataFrame()\n",
    "for file in files:\n",
    "    temp = pd.read_csv(file)\n",
    "    users = users.append(temp)\n",
    "users = users.rename(columns={'p.title':'title','s.display_name':'subreddit','p.score':'score','u.username':'username'})\n",
    "\n",
    "#users = users.iloc[0:50,:] # Filter to reduce the size of the user short list\n",
    "subreddit_driven = users # subreddit_driven  is used later to be added to user_driven\n",
    "print(\"Users list shape:\", users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save data to directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "users.to_csv('subreddit_driven.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 user driven approach: \n",
    "Return the list of users that have posted in all three subreddits, Of this list, take those with the highest karma.\n",
    "There will be some overlap in users between the subreddit driven approach and the user driven approach which is good. The former emphasize highly contributing members within each community, while the latter emphasizes on members that connect communities (with or without reposts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEO4J QUERY**\n",
    "```\n",
    "match (sr:Subreddit)--(p:Post)--(u:User)\n",
    "where sr.display_name in ['coronavirus', 'covid19', 'china_flu']\n",
    "and u.username <> 'AutoModerator'\n",
    "with distinct u as myUsers \n",
    "\n",
    "match (myUsers:User)--(pAny:Post)--(srAny:Subreddit)\n",
    "where srAny.display_name in ['europe', 'lifeprotips', 'science', 'videos', 'technology', 'iama', 'todayilearned', 'coronavirus', 'askreddit', 'explainlikeimfive', 'news', 'china_flu', 'covid19', 'nottheonion', 'politics', 'upliftingnews', 'askscience', 'worldnews', 'dataisbeautiful']\n",
    "with myUsers as my_N_Users, count(distinct pAny) as cntDiffPosts\n",
    "order by cntDiffPosts desc, my_N_Users.username limit 300\n",
    "\n",
    "return my_N_Users.username\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users shape: (300, 2)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "\n",
    "file = 'user_driven.csv'\n",
    "users = pd.read_csv(file)    \n",
    "users = users.rename(columns={'my_N_Users':'username','cntDiffPosts':'post_count'})\n",
    "\n",
    "#users = users.iloc[0:60,:] # Filter to reduce the size of the user short list\n",
    "user_driven = users # user_driven is used later to be added to subreddit_driven\n",
    "\n",
    "print(\"Users shape:\", users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER LIST SUMMARY:\n",
      "Length of the user driven approach list: 300\n",
      "Length of the subreddit driven approach list: 300\n",
      "Length of both lists: 600\n",
      "Length of both lists when removing duplicates512\n"
     ]
    }
   ],
   "source": [
    "# Complete list containing users of user_driven and subreddit_driven\n",
    "users_master_list = user_driven.loc[:,['username']].append(subreddit_driven.loc[:,['username']])\n",
    "\n",
    "# Summaries of both user lists\n",
    "print(\"USER LIST SUMMARY:\")\n",
    "print('Length of the user driven approach list: ' + str(len(user_driven.loc[:,['username']])))\n",
    "print('Length of the subreddit driven approach list: ' + str(len(subreddit_driven.loc[:,['username']])))\n",
    "print('Length of both lists: ' + str(len(users_master_list)))\n",
    "print('Length of both lists when removing duplicates' + str(len(set(users_master_list.values.flatten()))))\n",
    "\n",
    "# Removing duplicates \n",
    "users_master_list = list(set(users_master_list.values.flatten()))\n",
    "users_master_list = pd.Series(users_master_list)\n",
    "\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "users_master_list.to_csv('users_master_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "usernames = list(set(users_master_list.values.flatten())) # print this to get the full list in Neo4J compatible format\n",
    "print(len(usernames))\n",
    "#print(usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 posts:\n",
    "Reduce the amount of posts while retaining useful information. we export all the posts made by our shortlist of users for the three main coronavirus subreddits and our shortlist of subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEO4J QUERIES:**\n",
    "\n",
    "**coronvirus**\n",
    "```\n",
    "MATCH (u:User)-[e2]-(p:Post)-[e1]-(s:Subreddit{display_name:'coronavirus'})\n",
    "WHERE u.username in //Add master list of users here\n",
    "RETURN u.id, u.username, u.link_karma, u.comment_karma , s.id, s.display_name,p.id, p.created_utc_str, p.score, p.upvote_ratio ,p.title\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 10000\n",
    "```\n",
    "\n",
    "\n",
    "**covid19**\n",
    "```\n",
    "MATCH (u:User)-[e2]-(p:Post)-[e1]-(s:Subreddit{display_name:'covid19'})\n",
    "WHERE u.username in //Add master list of users here\n",
    "RETURN u.id, u.username, u.link_karma, u.comment_karma , s.id, s.display_name,p.id, p.created_utc_str, p.score, p.upvote_ratio ,p.title\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 10000\n",
    "```\n",
    "\n",
    "**china_flu**\n",
    "```\n",
    "MATCH (u:User)-[e2]-(p:Post)-[e1]-(s:Subreddit{display_name:'china_flu'})\n",
    "WHERE u.username in //Add master list of users here\n",
    "RETURN u.id, u.username, u.link_karma, u.comment_karma , s.id, s.display_name,p.id, p.created_utc_str, p.score, p.upvote_ratio ,p.title\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 10000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37386, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['u.id', 'username', 'link_karma', 'comment_karma', 's.id', 'subreddit',\n",
       "       'p.id', 'date', 'score', 'upvote_ratio', 'title', 'month', 'month_n',\n",
       "       'day', 'day_y', 'day_w', 'week_y', 'total_posts_per_user',\n",
       "       'total_score_per_user', 'total_posts_per_subreddit',\n",
       "       'total_scores_per_subreddit', 'average_karma_per_post',\n",
       "       'Time Interval'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Setting the directories\n",
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\posts'\n",
    "INPUT_DIR = '\\\\input'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "VIS_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\outputs\\\\exploratory_analysis\\\\posts\\\\'\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "\n",
    "# Loading the data from all subreddits\n",
    "files = ['china_flu_complete', 'coronavirus_complete', 'covid19_complete','technology',  'nottheonion',  'videos',  'politics',  'dataisbeautiful',  'askscience',  'news',  'askreddit',  'worldnews',  'todayilearned',  'upliftingnews',  'science',  'explainlikeimfive',  'europe']\n",
    "posts = pd.DataFrame()\n",
    "for file in files:\n",
    "    temp = pd.read_csv(file)\n",
    "    posts = posts.append(temp)\n",
    "\n",
    "#Cleaning posts dataframe\n",
    "posts = posts.rename(columns={'p.title':'title','s.display_name':'subreddit','p.score':'score','u.username':'username','u.link_karma':'link_karma','u.comment_karma':'comment_karma', 'p.created_utc_str':'date','p.upvote_ratio':'upvote_ratio'})\n",
    "posts = dates(posts)\n",
    "\n",
    "#Groupby's \n",
    "# Users\n",
    "total_posts_per_user = posts.loc[:,['username','title']].groupby('username').count().sort_values(by='title',  ascending=False)\n",
    "total_posts_per_user = total_posts_per_user.rename(columns={'title':'total_posts_per_user'}).reset_index() \n",
    "total_scores_per_user = posts.loc[:,['username','score']].groupby('username').sum().sort_values(by='score', ascending=False).reset_index() \n",
    "total_scores_per_user = total_scores_per_user.rename(columns={'score':'total_score_per_user'})\n",
    "\n",
    "#Subreddit \n",
    "total_posts_per_subreddit = posts.loc[:,['subreddit','title']].groupby('subreddit').count().sort_values(by='title',  ascending=False)\n",
    "total_posts_per_subreddit = total_posts_per_subreddit.rename(columns={'title':'total_posts_per_subreddit'}).reset_index() \n",
    "total_scores_per_subreddit = posts.loc[:,['subreddit','score']].groupby('subreddit').sum().sort_values(by='score', ascending=False).reset_index() \n",
    "total_scores_per_subreddit = total_scores_per_subreddit.rename(columns={'score':'total_scores_per_subreddit'})\n",
    "\n",
    "posts = pd.merge(left=posts,right=total_posts_per_user,how='left',left_on='username',right_on='username')\n",
    "cposts = pd.merge(left=posts,right=total_posts_per_subreddit,how='left',left_on='subreddit',right_on='subreddit')\n",
    "posts = pd.merge(left=posts,right=total_scores_per_subreddit,how='left',left_on='subreddit',right_on='subreddit')\n",
    "posts['average_karma_per_post'] = posts.loc[:,'total_score_per_user'] / posts.loc[:,'total_posts_per_user']\n",
    "posts.columns\n",
    "\n",
    "\n",
    "#Saving Dataframe\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "#posts = posts.loc[posts.username.isin(values=usernames)] # Special filter on shorter user list\n",
    "posts['Time Interval'] = posts.loc[:,'day_y']\n",
    "posts.to_csv('complete_graph.csv')\n",
    "print(posts.shape)\n",
    "posts.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final DataFrame containing topics**\n",
    "\n",
    "The LDA topic modeling was done elsewhere. It used the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u.id</th>\n",
       "      <th>username</th>\n",
       "      <th>link_karma</th>\n",
       "      <th>comment_karma</th>\n",
       "      <th>s.id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>p.id</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>day_w</th>\n",
       "      <th>week_y</th>\n",
       "      <th>total_posts_per_user</th>\n",
       "      <th>total_score_per_user</th>\n",
       "      <th>total_posts_per_subreddit</th>\n",
       "      <th>total_scores_per_subreddit</th>\n",
       "      <th>average_karma_per_post</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2o0ngbg8</td>\n",
       "      <td>goddessofthebitches</td>\n",
       "      <td>8376</td>\n",
       "      <td>1346</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>fhxap2</td>\n",
       "      <td>2020-03-13T10:05:33Z</td>\n",
       "      <td>16818</td>\n",
       "      <td>0.85</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>16818</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>16818.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>American Politics and News</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tblkklw</td>\n",
       "      <td>DeWallenVanWimKok</td>\n",
       "      <td>21356</td>\n",
       "      <td>6296</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>f0p5nc</td>\n",
       "      <td>2020-02-08T08:59:21Z</td>\n",
       "      <td>11927</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>157</td>\n",
       "      <td>27710</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>176.496815</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>Medical Research and Vaccine</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14mosq</td>\n",
       "      <td>staplehill</td>\n",
       "      <td>21357</td>\n",
       "      <td>42254</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>fbt49e</td>\n",
       "      <td>2020-03-01T12:01:27Z</td>\n",
       "      <td>8922</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>12187</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>870.500000</td>\n",
       "      <td>0.103280</td>\n",
       "      <td>Medical Research and Vaccine</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14bafgnj</td>\n",
       "      <td>IcyPresence96</td>\n",
       "      <td>4160</td>\n",
       "      <td>535</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>feupgd</td>\n",
       "      <td>2020-03-07T12:49:04Z</td>\n",
       "      <td>7043</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7074</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>1768.500000</td>\n",
       "      <td>0.232119</td>\n",
       "      <td>American Politics and News</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wo4fw</td>\n",
       "      <td>madman320</td>\n",
       "      <td>48415</td>\n",
       "      <td>22054</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>f30lyn</td>\n",
       "      <td>2020-02-12T23:49:03Z</td>\n",
       "      <td>6811</td>\n",
       "      <td>0.96</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>168</td>\n",
       "      <td>33525</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>199.553571</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>Statistics Reporting</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       u.id             username  link_karma  comment_karma    s.id  \\\n",
       "0  2o0ngbg8  goddessofthebitches        8376           1346  2dar36   \n",
       "1   tblkklw    DeWallenVanWimKok       21356           6296  2dar36   \n",
       "2    14mosq           staplehill       21357          42254  2dar36   \n",
       "3  14bafgnj        IcyPresence96        4160            535  2dar36   \n",
       "4     wo4fw            madman320       48415          22054  2dar36   \n",
       "\n",
       "   subreddit    p.id                  date  score  upvote_ratio     ...       \\\n",
       "0  china_flu  fhxap2  2020-03-13T10:05:33Z  16818          0.85     ...        \n",
       "1  china_flu  f0p5nc  2020-02-08T08:59:21Z  11927          0.95     ...        \n",
       "2  china_flu  fbt49e  2020-03-01T12:01:27Z   8922          0.98     ...        \n",
       "3  china_flu  feupgd  2020-03-07T12:49:04Z   7043          0.91     ...        \n",
       "4  china_flu  f30lyn  2020-02-12T23:49:03Z   6811          0.96     ...        \n",
       "\n",
       "  day_w week_y  total_posts_per_user total_score_per_user  \\\n",
       "0     4     11                     1                16818   \n",
       "1     5      6                   157                27710   \n",
       "2     6      9                    14                12187   \n",
       "3     5     10                     4                 7074   \n",
       "4     2      7                   168                33525   \n",
       "\n",
       "   total_posts_per_subreddit  total_scores_per_subreddit  \\\n",
       "0                       9515                      804324   \n",
       "1                       9515                      804324   \n",
       "2                       9515                      804324   \n",
       "3                       9515                      804324   \n",
       "4                       9515                      804324   \n",
       "\n",
       "   average_karma_per_post  avg_sentiment                         topic  \\\n",
       "0            16818.000000       0.000000    American Politics and News   \n",
       "1              176.496815       0.150000  Medical Research and Vaccine   \n",
       "2              870.500000       0.103280  Medical Research and Vaccine   \n",
       "3             1768.500000       0.232119    American Politics and News   \n",
       "4              199.553571       0.374166          Statistics Reporting   \n",
       "\n",
       "   topic_score  \n",
       "0         57.0  \n",
       "1         47.0  \n",
       "2         33.0  \n",
       "3         64.0  \n",
       "4         73.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the directories\n",
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\posts'\n",
    "INPUT_DIR = '\\\\input\\\\'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "import os\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "final_graph = pd.read_csv('reddit_posts_with_topic_and_sentiment_4.csv',encoding='iso-8859-1')\n",
    "final_graph.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "final_graph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gephi converter\n",
    "To create the final graph we need to convert our data to csv. We will need two types of files, csv containing the node infromation and csv's containing the relationship information. Below we create a csv for each\n",
    "\n",
    "\n",
    "## Nodes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delah\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "# Creating the nodes dataframe\n",
    "nodes = final_graph.copy()\n",
    "\n",
    "#Processing nodes\n",
    "# Post nodes\n",
    "nodes_post = nodes.loc[:,['p.id','title','date','score','upvote_ratio','month','month_n','day','Time Interval','day_w','week_y','avg_sentiment', 'topic', 'topic_score']]\n",
    "nodes_post = nodes_post.rename(columns={'p.id':'Id','title':'Label'})\n",
    "nodes_post.loc[:,'Label'] = ' '\n",
    "nodes_post['node_type'] = 'post'\n",
    "nodes_post = nodes_post.set_index(keys='Id')\n",
    "\n",
    "\n",
    "# User nodes\n",
    "nodes_user = nodes.loc[:,['u.id','username','link_karma','comment_karma','total_posts_per_user','Time Interval','total_score_per_user','average_karma_per_post']]\n",
    "nodes_user = nodes_user.rename(columns={'u.id':'Id','username':'Label'})\n",
    "nodes_user['node_type'] = 'user'\n",
    "nodes_user = nodes_user.set_index(keys='Id')\n",
    "# Subreddit nodes\n",
    "nodes_subreddit = nodes.loc[:,['s.id','subreddit','total_posts_per_subreddit','Time Interval','total_scores_per_subreddit']]\n",
    "nodes_subreddit = nodes_subreddit.rename(columns={'s.id':'Id','subreddit':'Label'})\n",
    "nodes_subreddit['node_type'] = 'subreddit'\n",
    "nodes_subreddit = nodes_subreddit.set_index(keys='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30337, 14)\n",
      "(30337, 8)\n",
      "(30337, 5)\n",
      "(30337, 3)\n",
      "(30337, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating the relationships dataframe\n",
    "relationships = final_graph.loc[:,['u.id','s.id','p.id']]\n",
    "\n",
    "# Processing relationships\n",
    "# User to post relationships\n",
    "rel_user_to_post = nodes.loc[:,['u.id','p.id']]\n",
    "rel_user_to_post['Submitted'] = 'Submitted'\n",
    "rel_user_to_post = rel_user_to_post.rename(columns={'u.id':'Source','p.id':'Target'})\n",
    "\n",
    "# Post to Subreddit relationships\n",
    "rel_post_to_subreddit = nodes.loc[:,['p.id','s.id']]\n",
    "rel_post_to_subreddit['Submitted'] = 'Submitted'\n",
    "rel_post_to_subreddit = rel_post_to_subreddit.rename(columns={'p.id':'Source','s.id':'Target'})\n",
    "\n",
    "\n",
    "print(nodes_post.shape)\n",
    "print(nodes_user.shape)\n",
    "print(nodes_subreddit.shape)\n",
    "print(rel_user_to_post.shape)\n",
    "print(rel_post_to_subreddit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\gephi'\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR)\n",
    "\n",
    "nodes_post.to_csv('post_nodes.csv')\n",
    "nodes_user.to_csv('user_nodes.csv')\n",
    "nodes_subreddit.to_csv('subreddit_nodes.csv')\n",
    "\n",
    "rel_user_to_post.to_csv('user_to_post_relationships.csv')\n",
    "rel_post_to_subreddit.to_csv('post_to_subreddit_relationships.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEO4J QUERIES\n",
    "## Coronavirus trends\n",
    "\n",
    "**askreddit**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'askreddit'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**science**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'science'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**worldnews**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'worldnews'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**videos**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'videos'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**todayilearned**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'todayilearned'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**news**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'news'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**iama**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'iama'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**askscience**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'askscience'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**explainlikeimfive**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'explainlikeimfive'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**lifeprotips**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'lifeprotips'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**nottheonion**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'nottheonion'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**upliftingnews**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'upliftingnews'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**dataisbeautiful**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'dataisbeautiful'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**technology**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'technology'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**politics**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'politics'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**europe**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'europe'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**coronavirus**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'coronavirus'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**covid19**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'covid19'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**china_flu**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'china_flu'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
